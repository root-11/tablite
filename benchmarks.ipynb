{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "These benchmarks seek to establish the performance of tablite as a user sees it.\n",
    "\n",
    "Overview\n",
    "\n",
    "**Input/Output:**\n",
    "\n",
    "- Save / Load .tpz format\n",
    "- Save tables to various formats\n",
    "- Import data from various formats\n",
    "\n",
    "**Various column functions:**\n",
    "\n",
    "- Setitem / getitem\n",
    "- iter\n",
    "- equal, not equal\n",
    "- copy\n",
    "- t += t\n",
    "- t *= t\n",
    "- contains\n",
    "- remove all\n",
    "- replace\n",
    "- index\n",
    "- unique\n",
    "- histogram\n",
    "- statistics\n",
    "- count\n",
    "\n",
    "\n",
    "**Various table functions**\n",
    "\n",
    "- **base**\n",
    "  - Setitem / getitem\n",
    "  - iter / rows\n",
    "  - equal, not equal\n",
    "  - load\n",
    "  - save\n",
    "  - copy\n",
    "  - stack\n",
    "  - types\n",
    "  - display_dict\n",
    "  - show\n",
    "  - to_dict\n",
    "  - as_json_serializable\n",
    "  - index\n",
    "- **core**\n",
    "  - expression\n",
    "  - filter\n",
    "  - sort_index\n",
    "  - reindex\n",
    "  - drop_duplicates\n",
    "  - sort\n",
    "  - is_sorted\n",
    "  - any\n",
    "  - all\n",
    "  - drop \n",
    "  - replace\n",
    "  - groupby\n",
    "  - pivot\n",
    "  - joins\n",
    "  - lookup\n",
    "  - replace missing values\n",
    "  - transpose\n",
    "  - pivot_transpose\n",
    "  - diff\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tablite import Table\n",
    "from tablite.datasets import synthetic_order_data\n",
    "import psutil, os, gc\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from time import process_time\n",
    "from tablite.config import Config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tables from synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1,000,000 rows is 240 Mb on disk, using 7 Mb ram\n",
      "Table 2,000,000 rows is 480 Mb on disk, using 15 Mb ram\n",
      "Table 5,000,000 rows is 1,200 Mb on disk, using 32 Mb ram\n",
      "Table 10,000,000 rows is 2,400 Mb on disk, using 27 Mb ram\n",
      "Table 50,000,000 rows is 12,000 Mb on disk, using 5 Mb ram\n",
      "Table 100,000,000 rows is 24,000 Mb on disk, using 7 Mb ram\n",
      "+===========+=======+=============+===================+=====+===+=====+====+===+====+===+===================+==================+\n",
      "|     ~     |   #   |      1      |         2         |  3  | 4 |  5  | 6  | 7 | 8  | 9 |         10        |        11        |\n",
      "+-----------+-------+-------------+-------------------+-----+---+-----+----+---+----+---+-------------------+------------------+\n",
      "|          0|      1|1150588223408|2021-12-24T00:00:00|50261|  0|18436|C4-2|EBL|0°  |XYZ| 1.6769818942324517|7.541014381548823 |\n",
      "|          1|      2| 142489783694|2021-12-05T00:00:00|50245|  1|17150|C1-3|KXT|21° |XYZ| 0.7794991289073795|3.446270878194184 |\n",
      "|          2|      3|1244663899590|2021-11-19T00:00:00|50692|  0| 8960|C2-2|VQF|6°  |XYZ| 0.3411609663561982|22.555150769749083|\n",
      "|          3|      4| 983502655425|2021-12-10T00:00:00|50066|  0|18902|C3-2|AWM|0°  |ABC| 0.2403129944912029|4.132472652840695 |\n",
      "|          4|      5|1428303638619|2021-11-14T00:00:00|50467|  0|16119|C3-5|RDI|6°  |ABC| 2.4739717934918914|5.801116929940667 |\n",
      "|          5|      6|1365714807870|2021-09-15T00:00:00|50776|  0|10992|C5-3|TPF|None|ABC| 2.0923336849039096|19.419200367488916|\n",
      "|          6|      7| 499246490602|2021-10-31T00:00:00|50098|  1| 4832|C1-4|AKM|21° |ABC| 1.3483459303897434|24.412414438675608|\n",
      "|...        |...    |...          |...                |...  |...|...  |... |...|... |...|...                |...               |\n",
      "| 99,999,993| 999994|1711334369587|2021-09-14T00:00:00|50093|  1|11086|C2-1|NUQ|0°  |   | 1.1528801128068211|16.05477772535433 |\n",
      "| 99,999,994| 999995|1822495281893|2021-09-15T00:00:00|50200|  1|28460|C2-1|MUA|None|ABC| 1.4240178382490014|5.013761086849668 |\n",
      "| 99,999,995| 999996| 163153891333|2021-09-15T00:00:00|50322|  0|20769|C5-1|RNN|6°  |XYZ|0.23926534082021525|21.771584915676318|\n",
      "| 99,999,996| 999997| 592268463172|2021-10-04T00:00:00|50464|  0| 7866|C3-5|HQD|None|XYZ|0.47444344244438397|6.942551962729615 |\n",
      "| 99,999,997| 999998| 294619023329|2021-11-15T00:00:00|50053|  0|24794|C1-2|OSD|21° |   | 0.9073992119813226|15.317018225220233|\n",
      "| 99,999,998| 999999| 965343936863|2021-11-27T00:00:00|50941|  0|27926|C1-5|RQU|None|XYZ| 0.9294879518747604|21.88173787256036 |\n",
      "| 99,999,999|1000000|1036582027359|2021-11-06T00:00:00|50419|  0|19952|C3-5|JZD|0°  |   | 0.6098196558104761|16.476310402584208|\n",
      "+===========+=======+=============+===================+=====+===+=====+====+===+====+===+===================+==================+\n"
     ]
    }
   ],
   "source": [
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "# The last tables are too big for RAM (~24Gb), so I create subtables of 1M rows and append them.\n",
    "ram_start = process.memory_info().rss\n",
    "t = synthetic_order_data(Config.PAGE_SIZE)\n",
    "ram_end = process.memory_info().rss\n",
    "real, flat = t.nbytes()\n",
    "print(f\"Table {len(t):,} rows is {real/1e6:,.0f} Mb on disk, using {(ram_end - ram_start)/1e6:,.0f} Mb ram\")\n",
    "\n",
    "tables = [t]  # 1M rows.\n",
    "\n",
    "for i in [2,5,10,50,100]:\n",
    "    for _ in range(10):\n",
    "        gc.collect()\n",
    "\n",
    "    ram_start = process.memory_info().rss\n",
    "    t2 = synthetic_order_data(Config.PAGE_SIZE)\n",
    "    for _ in range(i-1):\n",
    "        t2 += synthetic_order_data(Config.PAGE_SIZE)  # these are all unique\n",
    "    ram_end = process.memory_info().rss\n",
    "    real, flat = t2.nbytes()\n",
    "    tables.append(t2)\n",
    "    print(f\"Table {len(t2):,} rows is {real/1e6:,.0f} Mb on disk, using {(ram_end - ram_start)/1e6:,.0f} Mb ram\")\n",
    "\n",
    "tables[-1].show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save / Load .tpz format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving 1,000,000 rows (240 Mb) took 0.296875 secconds\n",
      "loading 1,000,000 tows took 0.609375 seconds\n",
      "saving 2,000,000 rows (480 Mb) took 0.71875 secconds\n",
      "loading 2,000,000 tows took 1.015625 seconds\n",
      "saving 5,000,000 rows (1,200 Mb) took 1.59375 secconds\n",
      "loading 5,000,000 tows took 2.65625 seconds\n",
      "saving 10,000,000 rows (2,400 Mb) took 3.359375 secconds\n",
      "loading 10,000,000 tows took 5.328125 seconds\n",
      "saving 50,000,000 rows (12,000 Mb) took 16.03125 secconds\n",
      "loading 50,000,000 tows took 29.453125 seconds\n",
      "saving 100,000,000 rows (24,000 Mb) took 31.890625 secconds\n",
      "loading 100,000,000 tows took 60.75 seconds\n"
     ]
    }
   ],
   "source": [
    "tmp = Path(tempfile.gettempdir()) / \"junk\"\n",
    "tmp.mkdir(exist_ok=True)\n",
    "\n",
    "for t in tables:\n",
    "    fn = tmp / f'{len(t)}.tpz'\n",
    "    start = process_time()\n",
    "    t.save(fn)\n",
    "    end = process_time()\n",
    "    assert fn.exists()\n",
    "    print(f\"saving {len(t):,} rows ({fn.stat().st_size/1e6:,.0f} Mb) took {end-start:,} secconds\")\n",
    "    \n",
    "    start = process_time()\n",
    "    t2 = Table.load(fn)\n",
    "    end = process_time()\n",
    "    print(f\"loading {len(t2):,} tows took {end-start:,} seconds\")\n",
    "    del t2\n",
    "    fn.unlink()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save / load tables to / from various formats\n",
    "\n",
    "The handlers for saving / export are:\n",
    "\n",
    "- to_sql\n",
    "- to_json\n",
    "- to_xls\n",
    "- to_ods\n",
    "- to_csv\n",
    "- to_tsv\n",
    "- to_text\n",
    "- to_html\n",
    "- to_hdf5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = synthetic_order_data(100_000)\n",
    "tmp = Path(tempfile.gettempdir()) / \"junk\"\n",
    "tmp.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_sql() took 1.31 secs for 100,000 rows\n"
     ]
    }
   ],
   "source": [
    "start = process_time()\n",
    "string = t.to_sql(name='t')  # --> SQL\n",
    "end = process_time()\n",
    "print(f\"to_sql() took {end-start:,.2f} secs for {len(t):,} rows\")\n",
    "\n",
    "# start = process_time() TODO\n",
    "# Table.from_sql(string)  # <-- SQL\n",
    "# end = process_time()\n",
    "# print(f\"from_sql() took {end-start:,.2f} secs for {len(t):,} rows\")\n",
    "del string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_json() took 1.42 secs for 100,000 rows\n",
      "from_json() took 0.44 secs for 100,000 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = process_time()\n",
    "bytestr = t.to_json()  # --> JSON\n",
    "end = process_time()\n",
    "print(f\"to_json() took {end-start:,.2f} secs for {len(t):,} rows\")\n",
    "\n",
    "start = process_time()\n",
    "Table.from_json(bytestr)  # <-- JSON\n",
    "end = process_time()\n",
    "print(f\"from_json() took {end-start:,.2f} secs for {len(t):,} rows\")\n",
    "del bytestr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.to_xls(1.xlsx) took 12.27 secs for 100,000 rows and used 241Mb RAM\n",
      "Table.from_file(1.xlsx) took 25.81 secs for 100,000 rows and used 534Mb RAM\n",
      "The file was 9 Mb on disk\n"
     ]
    }
   ],
   "source": [
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "fn = tmp / '1.xlsx'  # --> XLS\n",
    "start = process_time()\n",
    "t.to_xlsx(fn)\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"t.to_xls({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "start = process_time()\n",
    "Table.from_file(fn, sheet=\"pyexcel_sheet1\")  # <-- XLS\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"Table.from_file({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "print(f\"The file was {fn.stat().st_size/1e6:,.0f} Mb on disk\")\n",
    "fn.unlink()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.to_ods(1.ods) took 76.47 secs for 100,000 rows and used 2,191Mb RAM\n",
      "Table.from_file(1.ods) took 74.25 secs for 100,000 rows and used 911Mb RAM\n",
      "The file was 8 Mb on disk\n"
     ]
    }
   ],
   "source": [
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "fn = tmp / '1.ods' # --> ODS\n",
    "start = process_time()\n",
    "snip = t[:100_000]\n",
    "snip.to_ods(fn)  # limit the memory footprint.\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"t.to_ods({fn.name}) took {end-start:,.2f} secs for {len(snip):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "start = process_time()\n",
    "Table.from_file(fn, sheet=\"pyexcel_sheet1\")  # <-- ODS\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"Table.from_file({fn.name}) took {end-start:,.2f} secs for {len(snip):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "print(f\"The file was {fn.stat().st_size/1e6:,.0f} Mb on disk\")\n",
    "fn.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 60061.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.to_csv(1.csv) took 1.70 secs for 100,000 rows and used 216Mb RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "importing: consolidating '1.csv': 100.00%|██████████| [00:05<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table.from_file(1.csv) took 5.70 secs for 100,000 rows and used 245Mb RAM\n",
      "The file was 11 Mb on disk\n"
     ]
    }
   ],
   "source": [
    "Config.MULTIPROCESSING_MODE = Config.FALSE\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "fn = tmp / '1.csv'  # --> CSV\n",
    "start = process_time()\n",
    "t.to_csv(fn)\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"t.to_csv({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "start = process_time()\n",
    "Table.from_file(fn)  # <-- CSV\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"Table.from_file({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "print(f\"The file was {fn.stat().st_size/1e6:,.0f} Mb on disk\")\n",
    "fn.unlink()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 55394.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.to_tsv(1.tsv) took 1.80 secs for 100,000 rows and used 231Mb RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "importing: consolidating '1.tsv': 100.00%|██████████| [00:05<00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table.from_file(1.tsv) took 5.92 secs for 100,000 rows and used 231Mb RAM\n",
      "The file was 11 Mb on disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Config.MULTIPROCESSING_MODE = Config.FALSE\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "fn = tmp / '1.tsv'  # --> TSV\n",
    "start = process_time()\n",
    "t.to_tsv(fn)\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"t.to_tsv({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "start = process_time()\n",
    "Table.from_file(fn)  # <-- TSV\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"Table.from_file({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "print(f\"The file was {fn.stat().st_size/1e6:,.0f} Mb on disk\")\n",
    "fn.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 58966.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.to_text(1.txt) took 1.69 secs for 100,000 rows and used 223Mb RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "importing: consolidating '1.txt': 100.00%|██████████| [00:05<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table.from_file(1.txt) took 5.45 secs for 100,000 rows and used 223Mb RAM\n",
      "The file was 11 Mb on disk\n"
     ]
    }
   ],
   "source": [
    "Config.MULTIPROCESSING_MODE = Config.FALSE\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "fn = tmp / '1.txt'  # --> TXT\n",
    "start = process_time()\n",
    "t.to_text(fn)\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"t.to_text({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "start = process_time()\n",
    "Table.from_file(fn)  # <-- TXT\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"Table.from_file({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "print(f\"The file was {fn.stat().st_size/1e6:,.0f} Mb on disk\")\n",
    "fn.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.to_html(1.html) took 1.19 secs for 100,000 rows and used 347Mb RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_html: 100%|██████████| 22694923/22694923 [00:00<00:00, 29954442.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table.from_file(1.html) took 0.77 secs for 100,000 rows and used 335Mb RAM\n",
      "The file was 23 Mb on disk\n"
     ]
    }
   ],
   "source": [
    "Config.MULTIPROCESSING_MODE = Config.FALSE\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "fn = tmp / '1.html'  # --> HTML\n",
    "start = process_time()\n",
    "t.to_html(fn)\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"t.to_html({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "start = process_time()\n",
    "Table.from_file(fn)  # <-- HTML\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"Table.from_file({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "print(f\"The file was {fn.stat().st_size/1e6:,.0f} Mb on disk\")\n",
    "fn.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing :, records to C:\\Users\\madsenbj\\AppData\\Local\\Temp\\junk\\1.hdf5\n",
      "writing C:\\Users\\madsenbj\\AppData\\Local\\Temp\\junk\\1.hdf5 to HDF5 done\n",
      "t.to_hdf5(1.hdf5) took 0.70 secs for 100,000 rows and used 335Mb RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Data\\venv\\tablite310\\lib\\site-packages\\numpy\\lib\\format.py:362: UserWarning: metadata on a dtype may be saved or ignored, but will raise if saved when read. Use another form of storage.\n",
      "  d['descr'] = dtype_to_descr(array.dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table.from_file(1.hdf5) took 0.36 secs for 100,000 rows and used 317Mb RAM\n",
      "The file was 30 Mb on disk\n"
     ]
    }
   ],
   "source": [
    "Config.MULTIPROCESSING_MODE = Config.FALSE\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "fn = tmp / '1.hdf5'  # --> HDF5\n",
    "start = process_time()\n",
    "t.to_hdf5(fn)\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"t.to_hdf5({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "start = process_time()\n",
    "Table.from_file(fn)  # <-- HDF5\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"Table.from_file({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "print(f\"The file was {fn.stat().st_size/1e6:,.0f} Mb on disk\")\n",
    "fn.unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tablite310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
