{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "These benchmarks seek to establish the performance of tablite as a user sees it.\n",
    "\n",
    "Overview\n",
    "\n",
    "**Input/Output:**\n",
    "\n",
    "- Save / Load .tpz format\n",
    "- Save tables to various formats\n",
    "- Import data from various formats\n",
    "\n",
    "**Various column functions:**\n",
    "\n",
    "- Setitem / getitem\n",
    "- iter\n",
    "- equal, not equal\n",
    "- copy\n",
    "- t += t\n",
    "- t *= t\n",
    "- contains\n",
    "- remove all\n",
    "- replace\n",
    "- index\n",
    "- unique\n",
    "- histogram\n",
    "- statistics\n",
    "- count\n",
    "\n",
    "\n",
    "**Various table functions**\n",
    "\n",
    "- **base**\n",
    "  - Setitem / getitem\n",
    "  - iter / rows\n",
    "  - equal, not equal\n",
    "  - load\n",
    "  - save\n",
    "  - copy\n",
    "  - stack\n",
    "  - types\n",
    "  - display_dict\n",
    "  - show\n",
    "  - to_dict\n",
    "  - as_json_serializable\n",
    "  - index\n",
    "- **core**\n",
    "  - expression\n",
    "  - filter\n",
    "  - sort_index\n",
    "  - reindex\n",
    "  - drop_duplicates\n",
    "  - sort\n",
    "  - is_sorted\n",
    "  - any\n",
    "  - all\n",
    "  - drop \n",
    "  - replace\n",
    "  - groupby\n",
    "  - pivot\n",
    "  - joins\n",
    "  - lookup\n",
    "  - replace missing values\n",
    "  - transpose\n",
    "  - pivot_transpose\n",
    "  - diff\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tablite import Table\n",
    "from tablite.datasets import synthetic_order_data\n",
    "import psutil, os, gc\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from time import process_time\n",
    "from tablite.config import Config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tables from synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1,000,000 rows is 240 Mb on disk, using 7 Mb ram\n",
      "Table 2,000,000 rows is 480 Mb on disk, using 15 Mb ram\n",
      "Table 5,000,000 rows is 1,200 Mb on disk, using 28 Mb ram\n",
      "Table 10,000,000 rows is 2,400 Mb on disk, using 27 Mb ram\n",
      "Table 50,000,000 rows is 12,000 Mb on disk, using 6 Mb ram\n",
      "Table 100,000,000 rows is 24,000 Mb on disk, using 7 Mb ram\n",
      "+===========+=======+=============+===================+=====+===+=====+====+===+====+===+===================+==================+\n",
      "|     ~     |   #   |      1      |         2         |  3  | 4 |  5  | 6  | 7 | 8  | 9 |         10        |        11        |\n",
      "+-----------+-------+-------------+-------------------+-----+---+-----+----+---+----+---+-------------------+------------------+\n",
      "|          0|      1|1398629815837|2021-11-22T00:00:00|50696|  0|19084|C4-5|NHA|0°  |   |0.14986566997128964|12.10455014963018 |\n",
      "|          1|      2|1538699944465|2021-11-19T00:00:00|50045|  1|20508|C1-4|YKC|0°  |XYZ|  1.426933632208182|19.600321009705745|\n",
      "|          2|      3|1429530642607|2021-11-20T00:00:00|50596|  1|12877|C2-4|GKA|0°  |ABC|  1.422395435905098|2.7417023157941642|\n",
      "|          3|      4|2111199257547|2021-08-22T00:00:00|50797|  1|16676|C3-2|CMP|0°  |XYZ| 2.1873364904059818|2.9039803588736524|\n",
      "|          4|      5| 764683194675|2021-09-29T00:00:00|50912|  0|11753|C1-4|JZW|21° |ABC| 1.2500068214180635|15.765268888491535|\n",
      "|          5|      6|1618920148907|2021-11-27T00:00:00|50517|  1| 8700|C2-1|NZI|None|XYZ| 1.0957858940271716|7.264965082932238 |\n",
      "|          6|      7|1649188866341|2021-08-29T00:00:00|50034|  0|27936|C3-5|QEL|6°  |   | 0.8469834107919338|10.466567413455955|\n",
      "|...        |...    |...          |...                |...  |...|...  |... |...|... |...|...                |...               |\n",
      "| 99,999,993| 999994|2258958029290|2021-12-05T00:00:00|50081|  1| 4810|C1-4|PKS|0°  |ABC| 0.6145458530611322|2.219792698371086 |\n",
      "| 99,999,994| 999995| 810886071341|2021-10-01T00:00:00|50120|  1|22554|C3-3|YGR|None|ABC| 1.7818522963322894|1.1568769561770718|\n",
      "| 99,999,995| 999996|1357570657397|2021-12-11T00:00:00|50207|  0|24674|C4-5|QTM|21° |ABC|0.18471237212830577|4.613316094886708 |\n",
      "| 99,999,996| 999997| 909446091956|2021-08-09T00:00:00|50472|  1|19179|C1-5|ADS|0°  |ABC|0.44009553846109734|19.882725917791962|\n",
      "| 99,999,997| 999998|2270445685075|2021-10-27T00:00:00|50169|  0| 4020|C4-5|XBB|21° |XYZ| 1.9312360021763941|20.26637572310398 |\n",
      "| 99,999,998| 999999|1095168039713|2021-09-18T00:00:00|50907|  1|18376|C4-4|BQN|None|XYZ| 0.2778417429197087|19.72498976075743 |\n",
      "| 99,999,999|1000000| 310970086217|2021-08-15T00:00:00|50634|  1|16323|C4-2|XXH|21° |ABC| 0.3424604548563899|4.488536564542899 |\n",
      "+===========+=======+=============+===================+=====+===+=====+====+===+====+===+===================+==================+\n"
     ]
    }
   ],
   "source": [
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "# The last tables are too big for RAM (~24Gb), so I create subtables of 1M rows and append them.\n",
    "ram_start = process.memory_info().rss\n",
    "t = synthetic_order_data(Config.PAGE_SIZE)\n",
    "ram_end = process.memory_info().rss\n",
    "real, flat = t.nbytes()\n",
    "print(f\"Table {len(t):,} rows is {real/1e6:,.0f} Mb on disk, using {(ram_end - ram_start)/1e6:,.0f} Mb ram\")\n",
    "\n",
    "tables = [t]  # 1M rows.\n",
    "\n",
    "for i in [2,5,10,50,100]:\n",
    "    for _ in range(10):\n",
    "        gc.collect()\n",
    "\n",
    "    ram_start = process.memory_info().rss\n",
    "    t2 = synthetic_order_data(Config.PAGE_SIZE)\n",
    "    for _ in range(i-1):\n",
    "        t2 += synthetic_order_data(Config.PAGE_SIZE)  # these are all unique\n",
    "    ram_end = process.memory_info().rss\n",
    "    real, flat = t2.nbytes()\n",
    "    tables.append(t2)\n",
    "    print(f\"Table {len(t2):,} rows is {real/1e6:,.0f} Mb on disk, using {(ram_end - ram_start)/1e6:,.0f} Mb ram\")\n",
    "\n",
    "tables[-1].show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save / Load .tpz format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving 1,000,000 rows (240 Mb) took 0.296875 secconds\n",
      "loading 1,000,000 tows took 0.5 seconds\n",
      "saving 2,000,000 rows (480 Mb) took 0.625 secconds\n",
      "loading 2,000,000 tows took 1.046875 seconds\n",
      "saving 5,000,000 rows (1,200 Mb) took 1.5 secconds\n",
      "loading 5,000,000 tows took 2.484375 seconds\n",
      "saving 10,000,000 rows (2,400 Mb) took 3.140625 secconds\n",
      "loading 10,000,000 tows took 4.84375 seconds\n",
      "saving 50,000,000 rows (12,000 Mb) took 15.765625 secconds\n",
      "loading 50,000,000 tows took 29.84375 seconds\n",
      "saving 100,000,000 rows (24,000 Mb) took 30.40625 secconds\n",
      "loading 100,000,000 tows took 57.8125 seconds\n"
     ]
    }
   ],
   "source": [
    "tmp = Path(tempfile.gettempdir()) / \"junk\"\n",
    "tmp.mkdir(exist_ok=True)\n",
    "\n",
    "for t in tables:\n",
    "    fn = tmp / f'{len(t)}.tpz'\n",
    "    start = process_time()\n",
    "    t.save(fn)\n",
    "    end = process_time()\n",
    "    assert fn.exists()\n",
    "    print(f\"saving {len(t):,} rows ({fn.stat().st_size/1e6:,.0f} Mb) took {end-start:,} secconds\")\n",
    "    \n",
    "    start = process_time()\n",
    "    t2 = Table.load(fn)\n",
    "    end = process_time()\n",
    "    print(f\"loading {len(t2):,} tows took {end-start:,} seconds\")\n",
    "    del t2\n",
    "    fn.unlink()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save / load tables to / from various formats\n",
    "\n",
    "The handlers for saving / export are:\n",
    "\n",
    "- to_sql\n",
    "- to_json\n",
    "- to_xls\n",
    "- to_ods\n",
    "- to_csv\n",
    "- to_tsv\n",
    "- to_text\n",
    "- to_html\n",
    "- to_hdf5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = synthetic_order_data(1_000_000)\n",
    "tmp = Path(tempfile.gettempdir()) / \"junk\"\n",
    "tmp.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_sql() took 12.91 secs for 1,000,000 rows\n"
     ]
    }
   ],
   "source": [
    "start = process_time()\n",
    "string = t.to_sql(name='t')  # --> SQL\n",
    "end = process_time()\n",
    "print(f\"to_sql() took {end-start:,.2f} secs for {len(t):,} rows\")\n",
    "\n",
    "# start = process_time() TODO\n",
    "# Table.from_sql(string)  # <-- SQL\n",
    "# end = process_time()\n",
    "# print(f\"from_sql() took {end-start:,.2f} secs for {len(t):,} rows\")\n",
    "del string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_json() took 12.83 secs for 1,000,000 rows\n",
      "from_json() took 3.81 secs for 1,000,000 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = process_time()\n",
    "bytestr = t.to_json()  # --> JSON\n",
    "end = process_time()\n",
    "print(f\"to_json() took {end-start:,.2f} secs for {len(t):,} rows\")\n",
    "\n",
    "start = process_time()\n",
    "Table.from_json(bytestr)  # <-- JSON\n",
    "end = process_time()\n",
    "print(f\"from_json() took {end-start:,.2f} secs for {len(t):,} rows\")\n",
    "del bytestr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.to_xls(1.xlsx) took 133.58 secs for 1,000,000 rows and used 1,856Mb RAM\n",
      "Table.from_file(1.xlsx) took 250.17 secs for 1,000,000 rows and used 5,153Mb RAM\n",
      "The file was 92 Mb on disk\n"
     ]
    }
   ],
   "source": [
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "fn = tmp / '1.xlsx'  # --> XLS\n",
    "start = process_time()\n",
    "t.to_xlsx(fn)\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"t.to_xls({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "start = process_time()\n",
    "Table.from_file(fn, sheet=\"pyexcel_sheet1\")  # <-- XLS\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"Table.from_file({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "print(f\"The file was {fn.stat().st_size/1e6:,.0f} Mb on disk\")\n",
    "fn.unlink()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.to_ods(1.ods) took 83.28 secs for 100,000 rows and used 1,005Mb RAM\n",
      "Table.from_file(1.ods) took 74.67 secs for 100,000 rows and used 1,642Mb RAM\n",
      "The file was 8 Mb on disk\n"
     ]
    }
   ],
   "source": [
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "fn = tmp / '1.ods' # --> ODS\n",
    "start = process_time()\n",
    "snip = t[:100_000]\n",
    "snip.to_ods(fn)  # limit the memory footprint.\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"t.to_ods({fn.name}) took {end-start:,.2f} secs for {len(snip):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "start = process_time()\n",
    "Table.from_file(fn, sheet=\"pyexcel_sheet1\")  # <-- ODS\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"Table.from_file({fn.name}) took {end-start:,.2f} secs for {len(snip):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "print(f\"The file was {fn.stat().st_size/1e6:,.0f} Mb on disk\")\n",
    "fn.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:16<00:00, 59673.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.to_csv(1.csv) took 16.83 secs for 1,000,000 rows and used 3,124Mb RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "importing: consolidating '1.csv': 100.00%|██████████| [00:59<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table.from_file(1.csv) took 59.11 secs for 1,000,000 rows and used 3,153Mb RAM\n",
      "The file was 109 Mb on disk\n"
     ]
    }
   ],
   "source": [
    "Config.MULTIPROCESSING_MODE = Config.FALSE\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "fn = tmp / '1.csv'  # --> CSV\n",
    "start = process_time()\n",
    "t.to_csv(fn)\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"t.to_csv({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "start = process_time()\n",
    "Table.from_file(fn)  # <-- CSV\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"Table.from_file({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "print(f\"The file was {fn.stat().st_size/1e6:,.0f} Mb on disk\")\n",
    "fn.unlink()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:16<00:00, 59532.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.to_tsv(1.tsv) took 16.84 secs for 1,000,000 rows and used 3,034Mb RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "importing: consolidating '1.tsv': 100.00%|██████████| [00:58<00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table.from_file(1.tsv) took 58.31 secs for 1,000,000 rows and used 3,034Mb RAM\n",
      "The file was 109 Mb on disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Config.MULTIPROCESSING_MODE = Config.FALSE\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "fn = tmp / '1.tsv'  # --> TSV\n",
    "start = process_time()\n",
    "t.to_tsv(fn)\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"t.to_tsv({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "start = process_time()\n",
    "Table.from_file(fn)  # <-- TSV\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"Table.from_file({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "print(f\"The file was {fn.stat().st_size/1e6:,.0f} Mb on disk\")\n",
    "fn.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:16<00:00, 59597.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.to_text(1.txt) took 16.83 secs for 1,000,000 rows and used 3,023Mb RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "importing: consolidating '1.txt': 100.00%|██████████| [00:57<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table.from_file(1.txt) took 57.69 secs for 1,000,000 rows and used 3,023Mb RAM\n",
      "The file was 109 Mb on disk\n"
     ]
    }
   ],
   "source": [
    "Config.MULTIPROCESSING_MODE = Config.FALSE\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "fn = tmp / '1.txt'  # --> TXT\n",
    "start = process_time()\n",
    "t.to_text(fn)\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"t.to_text({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "start = process_time()\n",
    "Table.from_file(fn)  # <-- TXT\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"Table.from_file({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "print(f\"The file was {fn.stat().st_size/1e6:,.0f} Mb on disk\")\n",
    "fn.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.to_html(1.html) took 11.70 secs for 1,000,000 rows and used 3,330Mb RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_html: 100%|██████████| 228960213/228960213 [01:19<00:00, 2893933.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table.from_file(1.html) took 79.22 secs for 1,000,000 rows and used 3,329Mb RAM\n",
      "The file was 229 Mb on disk\n"
     ]
    }
   ],
   "source": [
    "Config.MULTIPROCESSING_MODE = Config.FALSE\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "fn = tmp / '1.html'  # --> HTML\n",
    "start = process_time()\n",
    "t.to_html(fn)\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"t.to_html({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "start = process_time()\n",
    "Table.from_file(fn)  # <-- HTML\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"Table.from_file({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "print(f\"The file was {fn.stat().st_size/1e6:,.0f} Mb on disk\")\n",
    "fn.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing :, records to C:\\Users\\madsenbj\\AppData\\Local\\Temp\\junk\\1.hdf5\n",
      "writing C:\\Users\\madsenbj\\AppData\\Local\\Temp\\junk\\1.hdf5 to HDF5 done\n",
      "t.to_hdf5(1.hdf5) took 6.08 secs for 1,000,000 rows and used 3,329Mb RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Data\\venv\\tablite310\\lib\\site-packages\\numpy\\lib\\format.py:362: UserWarning: metadata on a dtype may be saved or ignored, but will raise if saved when read. Use another form of storage.\n",
      "  d['descr'] = dtype_to_descr(array.dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table.from_file(1.hdf5) took 2.83 secs for 1,000,000 rows and used 3,289Mb RAM\n",
      "The file was 301 Mb on disk\n"
     ]
    }
   ],
   "source": [
    "Config.MULTIPROCESSING_MODE = Config.FALSE\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "fn = tmp / '1.hdf5'  # --> HDF5\n",
    "start = process_time()\n",
    "t.to_hdf5(fn)\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"t.to_hdf5({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "\n",
    "p = psutil.Process()\n",
    "start_ram = p.memory_full_info().uss\n",
    "start = process_time()\n",
    "Table.from_file(fn)  # <-- HDF5\n",
    "end = process_time()\n",
    "max_ram = p.memory_full_info().peak_wset\n",
    "print(f\"Table.from_file({fn.name}) took {end-start:,.2f} secs for {len(t):,} rows and used {(max_ram-start_ram)/1e6:,.0f}Mb RAM\")\n",
    "print(f\"The file was {fn.stat().st_size/1e6:,.0f} Mb on disk\")\n",
    "fn.unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tablite310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
